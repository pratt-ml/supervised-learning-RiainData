{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.linear_model import ElasticNetCV\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ucimlrepo import fetch_ucirepo \n",
    "  \n",
    "# fetch dataset \n",
    "wine_quality = fetch_ucirepo(id=186) \n",
    "  \n",
    "# data (as pandas dataframes) \n",
    "X = wine_quality.data.features \n",
    "y = wine_quality.data.targets \n",
    "  \n",
    "# metadata \n",
    "print(wine_quality.metadata) \n",
    "  \n",
    "# variable information \n",
    "print(wine_quality.variables) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wine_df = pd.concat([X, y], axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: Data exploration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wine_df.head"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wine_df.describe()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wine_df.isnull().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### there are no nulls in the dataset, so we can move onto looking at the correlation of features in the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "correlation_matrix = wine_df.corr()\n",
    "correlation_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10, 8))\n",
    "sns.heatmap(correlation_matrix, annot=True, cmap='coolwarm', fmt='.2f')\n",
    "plt.title('Correlation Heatmap')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assuming 'correlation_matrix' is a DataFrame containing the correlations\n",
    "print(correlation_matrix.to_string())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The analysis reveals several important correlations within the wine dataset. There is a strong positive correlation between free sulfur dioxide and total sulfur dioxide (0.720934), suggesting that these two sulfur dioxide variables likely increase together. Similarly, a significant positive correlation exists between residual sugar and density (0.552517), indicating that wines with higher residual sugar also tend to have higher density. On the other hand, a strong negative correlation is observed between alcohol and density (-0.686745), suggesting that as alcohol content increases, wine density tends to decrease. Additionally, there is a negative relationship between quality and volatile acidity (-0.265699), implying that wines with higher volatile acidity often have lower quality. Notably, alcohol content shows a positive correlation with quality (0.444319), indicating that higher alcohol levels are frequently associated with better wine quality. Conversely, fixed acidity, citric acid, chlorides, and residual sugar all exhibit weak correlations with quality, suggesting they may not be strong predictors of wine quality in the analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### The correlation analysis provides several implications for modeling a supervised learning algorithm in this wine dataset. Firstly, the strong correlations between certain features, such as free sulfur dioxide and total sulfur dioxide, suggest that these variables may provide redundant information, which could lead to multicollinearity issues in the model. This redundancy can be addressed by removing one of the correlated features or applying dimensionality reduction techniques to ensure a more stable model.\n",
    "\n",
    "#### Secondly, the identified relationships between features and the target variable (quality) can guide feature selection. For instance, the positive correlation between alcohol content and quality indicates that including alcohol as a predictor could improve model performance. In contrast, features with weak correlations to quality, like fixed acidity and citric acid, may be candidates for exclusion, simplifying the model without sacrificing accuracy.\n",
    "\n",
    "#### Additionally, the negative correlation between volatile acidity and quality suggests that it could be a crucial feature for predicting quality, prompting the need for further exploration of its impact. Overall, these insights will help in constructing a more effective supervised learning model by emphasizing relevant features, reducing complexity, and potentially improving predictive accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(wine_df.dtypes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### floats (numerical values) are suitable for training a supervised learning model. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Split the features (X) from the target variable (y).\n",
    "\n",
    "X = wine_df.drop(columns='quality')  # All features except 'quality'\n",
    "y = wine_df['quality']  # Target variable "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Split the Data\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: Modelling, Preprocessing and Pipeline creation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Logistic Regression Pipeline\n",
    "logistic_pipeline = Pipeline(steps=[\n",
    "    ('scaler', StandardScaler()),  # Step for scaling\n",
    "    ('logreg', LogisticRegression())  # Step for logistic regression model\n",
    "])\n",
    "\n",
    "# Random Forest Pipeline\n",
    "rf_pipeline = Pipeline(steps=[\n",
    "    ('scaler', StandardScaler()),  # Step for scaling\n",
    "    ('rf', RandomForestClassifier(random_state=42))  # Step for random forest model\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit the logistic regression pipeline\n",
    "logistic_pipeline.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit the random forest pipeline\n",
    "rf_pipeline.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Logistic Regression Predictions\n",
    "y_pred_log = logistic_pipeline.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Random Forest Predictions\n",
    "y_pred_rf = rf_pipeline.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Logistic Regression Evaluation\n",
    "print(\"Logistic Regression Classification Report:\")\n",
    "print(classification_report(y_test, y_pred_log))\n",
    "print(\"Confusion Matrix:\")\n",
    "print(confusion_matrix(y_test, y_pred_log))\n",
    "print(\"Accuracy Score:\", accuracy_score(y_test, y_pred_log))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Random Forest Evaluation\n",
    "print(\"Random Forest Classification Report:\")\n",
    "print(classification_report(y_test, y_pred_rf))\n",
    "print(\"Confusion Matrix:\")\n",
    "print(confusion_matrix(y_test, y_pred_rf))\n",
    "print(\"Accuracy Score:\", accuracy_score(y_test, y_pred_rf))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3: hyperparameter tuning \n",
    "\n",
    "### random forest seems to have had best F1 so we are taking that model to the hyperparameter tuning step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "param_grid = {\n",
    "\n",
    "    'rf__n_estimators': [100, 200],           # Number of trees\n",
    "    'rf__max_depth': [None, 10, 20],          # Maximum depth of the trees\n",
    "    'rf__min_samples_split': [2, 5, 10]       # Minimum number of samples required to split an internal node\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the grid search with the pipeline\n",
    "grid_search = GridSearchCV(estimator=rf_pipeline, \n",
    "                           param_grid=param_grid, \n",
    "                           cv=5,                      # 5-fold cross-validation\n",
    "                           scoring='f1',             # Metric to optimize\n",
    "                           n_jobs=-1)                # Use all available cores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grid_search.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print (\"Best Parameters:\", grid_search.best_params_)\n",
    "print (\"Best Cross-Validation Score:\", grid_search.best_score_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_params = grid_search.best_params_\n",
    "print(\"Best parameters found: \", best_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_model = grid_search.best_estimator_  \n",
    "test_score = best_model.score(X_test, y_test)\n",
    "print(\"Test score of the best model: \", test_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#reassigning \n",
    "\n",
    "y_pred_rf = best_model.predict(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Incorporating model in a function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def wine_quality(row, best_model):\n",
    "    \n",
    "      # Reshape the row to ensure itâ€™s a 2D array with one sample\n",
    "    row = np.array(row).reshape(1, -1)  \n",
    "    # Sickitlearn library seemed to be having issues reading the row. \n",
    "    # Full row of 11 features needs to be input in order to get a prediction from the function. \n",
    "    \n",
    "    # Use the model to predict wine quality\n",
    "    prediction = best_model.predict(row)\n",
    "    \n",
    "    # Return the predicted quality\n",
    "    return prediction[0]\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assuming row has 11 features\n",
    "sample_row = [7.4, 0.7, 0.0, 1.9, 0.076, 11.0, 31.0, 0.9978, 3.51, 0.56, 9.4]\n",
    "quality_prediction = wine_quality(sample_row, best_model)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "quality_prediction"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
